# Resource Group and Network
resource "azurerm_resource_group" "airflow" {
  name     = var.resource_group_name
  location = var.location
}

resource "azurerm_virtual_network" "airflow" {
  name                = var.vnet_name
  address_space       = ["10.10.0.0/16"]
  location            = azurerm_resource_group.airflow.location
  resource_group_name = azurerm_resource_group.airflow.name
}

resource "azurerm_subnet" "aks" {
  name                 = "subnet-aks"
  resource_group_name  = azurerm_resource_group.airflow.name
  virtual_network_name = azurerm_virtual_network.airflow.name
  address_prefixes     = ["10.10.0.0/22"]
}

resource "azurerm_subnet" "db" {
  name                 = "subnet-db-private"
  resource_group_name  = azurerm_resource_group.airflow.name
  virtual_network_name = azurerm_virtual_network.airflow.name
  address_prefixes     = ["10.10.4.0/24"]
}

resource "azurerm_private_dns_zone" "postgres" {
  name                = "privatelink.postgres.database.azure.com"
  resource_group_name = azurerm_resource_group.airflow.name
}

# PostgreSQL Flexible Server (private)
resource "azurerm_postgresql_flexible_server" "airflow_db" {
  name                = "airflow-metadb-postgresql"
  resource_group_name = azurerm_resource_group.airflow.name
  location            = azurerm_resource_group.airflow.location

  administrator_login    = var.db_admin_user
  administrator_password = var.db_admin_password
  version                = "16"

  sku_name   = "GP_Standard_D2ds_v5"
  storage_mb = 32768

  delegated_subnet_id = azurerm_subnet.db.id
  private_dns_zone_id = azurerm_private_dns_zone.postgres.id
  public_network_access_enabled = false

  authentication {
    password_auth_enabled = true
  }

  depends_on = [azurerm_subnet.db]
}

resource "azurerm_postgresql_flexible_server_database" "airflow_db_database" {
  name      = "airflow"
  server_id = azurerm_postgresql_flexible_server.airflow_db.id
  charset   = "UTF8"
  collation = "en_US.utf8"
}

# AKS cluster
resource "azurerm_kubernetes_cluster" "aks" {
  name                = "aks-airflow-prod"
  location            = azurerm_resource_group.airflow.location
  resource_group_name = azurerm_resource_group.airflow.name
  dns_prefix          = "airflow"

  default_node_pool {
    name                = "systempool"
    node_count          = 1
    vm_size             = "Standard_B2s"
    vnet_subnet_id      = azurerm_subnet.aks.id
  }

  identity {
    type = "SystemAssigned"
  }

  kubernetes_version = "1.30.3"
}

resource "azurerm_kubernetes_cluster_node_pool" "workerpool" {
  name                  = "workerpool"
  kubernetes_cluster_id = azurerm_kubernetes_cluster.aks.id
  vm_size               = "Standard_D4s_v3"
  node_count            = 0
  auto_scaling_enabled  = true
  min_count             = 0
  max_count             = 5
  mode                  = "User"

  node_taints = ["workload=worker:NoSchedule"]
}

# Public IP (in AKS node resource group)
resource "azurerm_public_ip" "airflow_web" {
  name                = "pip-airflow-web"
  location            = azurerm_resource_group.airflow.location
  resource_group_name = azurerm_kubernetes_cluster.aks.node_resource_group
  allocation_method   = "Static"
  sku                 = "Standard"
}

# Kubernetes namespace and secrets (using kubernetes provider configured from AKS kubeconfig)
resource "kubernetes_namespace" "airflow_ns" {
  metadata {
    name = "airflow"
  }

  depends_on = [azurerm_kubernetes_cluster.aks]
}

resource "kubernetes_secret" "airflow_db_secret" {
  provider = kubernetes.aks

  metadata {
    name      = "airflow-db-secret"
    namespace = kubernetes_namespace.airflow_ns.metadata[0].name
  }

  data = {
    connection = base64encode("postgresql://${var.db_admin_user}:${var.db_admin_password}@${azurerm_postgresql_flexible_server.airflow_db.fqdn}:5432/airflow")
  }

  depends_on = [kubernetes_namespace.airflow_ns]
}

resource "kubernetes_secret" "airflow_ssh_secret" {
  provider = kubernetes.aks

  metadata {
    name      = "airflow-ssh-secret"
    namespace = kubernetes_namespace.airflow_ns.metadata[0].name
  }

  data = {
    gitSshKey = base64encode(var.ssh_private_key)
  }

  depends_on = [kubernetes_namespace.airflow_ns]
}

resource "kubernetes_secret" "airflow_ssh_knownhosts" {
  provider = kubernetes.aks

  metadata {
    name      = "airflow-ssh-knownhosts"
    namespace = kubernetes_namespace.airflow_ns.metadata[0].name
  }

  data = {
    knownHosts = base64encode("github.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=")
  }

  depends_on = [kubernetes_namespace.airflow_ns]
}

# Helm release for Airflow
resource "helm_release" "airflow" {
You said:
crie e estruture todos esses comando no Terraform para executar no HCP
az group create \
  --name rg-airflow-prod \
  --location centralus

az network vnet create \
  --resource-group rg-airflow-prod \
  --name vnet-airflow \
  --address-prefixes 10.10.0.0/16

az network vnet subnet create \
  --resource-group rg-airflow-prod \
  --vnet-name vnet-airflow \
  --name subnet-aks \
  --address-prefixes 10.10.0.0/22

az network vnet subnet create \
  --resource-group rg-airflow-prod \
  --vnet-name vnet-airflow \
  --name subnet-db-private \
  --address-prefixes 10.10.4.0/24

az network private-dns zone create \
  --resource-group rg-airflow-prod \
  --name "privatelink.postgres.database.azure.com"

az postgres flexible-server create \
  --resource-group rg-airflow-prod \
  --name airflow-metadb-postgresql \
  --location centralus \
  --admin-user airflow_user \
  --admin-password "airflow_pass" \
  --sku-name Standard_D2ds_v5 \
  --tier GeneralPurpose \
  --version 16 \
  --storage-size 32 \
  --high-availability Disabled \
  --vnet vnet-airflow \
  --subnet subnet-db-private \
  --private-dns-zone "privatelink.postgres.database.azure.com" \
  --database-name airflow


az aks create \
  --resource-group rg-airflow-prod \
  --name aks-airflow-prod \
  --node-count 1 \
  --node-vm-size Standard_B2s \
  --enable-addons monitoring \
  --network-plugin azure \
  --vnet-subnet-id $(az network vnet subnet show --resource-group rg-airflow-prod --vnet-name vnet-airflow --name subnet-aks --query id -o tsv) \
  --enable-managed-identity \
  --kubernetes-version 1.30.3 \
  --generate-ssh-keys \
  --enable-cluster-autoscaler \
  --min-count 1 \
  --max-count 1 \
  --nodepool-name systempool
  
  
az aks nodepool add \
  --resource-group rg-airflow-prod \
  --cluster-name aks-airflow-prod \
  --name workerpool \
  --node-vm-size Standard_D4s_v3 \
  --node-count 0 \
  --enable-cluster-autoscaler \
  --min-count 0 \
  --max-count 5 \
  --mode User \
  --node-taints workload=worker:NoSchedule
  
  
  az aks get-credentials \
  --resource-group rg-airflow-prod \
  --name aks-airflow-prod



kubectl create secret generic airflow-db-secret \
  --namespace airflow \
  --from-literal=connection="postgresql://airflow_user:airflow_pass@airflow-metadb-postgresql.postgres.database.azure.com:5432/airflow"


helm repo add apache-airflow https://airflow.apache.org
helm repo update


az network public-ip create \
  --resource-group MC_rg-airflow-prod_aks-airflow-prod_centralus \
  --name pip-airflow-web \
  --sku Standard \
  --allocation-method Static

az network public-ip show -g MC_rg-airflow-prod_aks-airflow-prod_centralus -n pip-airflow-web --query ipAddress -o tsv


kubectl create namespace airflow

kubectl get secret airflow-db-secret -n airflow \
  -o jsonpath="{.data.connection}" | base64 --decode

kubectl patch svc airflow-webserver -n airflow -p '{"spec":{"type":"LoadBalancer","loadBalancerIP":"135.119.50.30"}}'

kubectl create secret generic airflow-ssh-knownhosts \
  --from-literal=knownHosts="$(ssh-keyscan github.com)" \
  --namespace airflow

helm upgrade --install airflow apache-airflow/airflow \
  --namespace airflow \
  --create-namespace \
  --version 1.16.0 \
  --set executor=KubernetesExecutor \
  --set postgresql.enabled=false \
  --set redis.enabled=false \
  --set data.metadataSecretName=airflow-db-secret \
  --set airflow.persistence.enabled=true \
  --set airflow.persistence.size=20Gi \
  --set images.airflow.repository=apache/airflow \
  --set images.airflow.tag=2.10.5 \
  --set airflow.airflowVersion=2.10.5 \
  --set webserver.enabled=true \
  --set webserver.service.type=LoadBalancer \
  --set webserver.service.loadBalancerIP=135.119.50.30 \
  --set webserver.livenessProbe.initialDelaySeconds=60 \
  --set webserver.readinessProbe.initialDelaySeconds=60 \
  --set webserver.livenessProbe.failureThreshold=20 \
  --set webserver.readinessProbe.failureThreshold=20 \
  --set dags.gitSync.enabled=true \
  --set-string dags.gitSync.repo=git@github.com:danielkakazu/dags.git \
  --set dags.gitSync.branch=main \
  --set dags.gitSync.subPath="dags" \
  --set dags.gitSync.sshKeySecret=airflow-ssh-secret \
  --set-string dags.gitSync.knownHosts="$KNOWN_HOSTS" \
  --set executorConfig.nodeSelector.agentpool=workerpool \
  --set executorConfig.tolerations[0].key="workerpool" \
  --set executorConfig.tolerations[0].operator="Exists" \
  --set executorConfig.tolerations[0].effect="NoSchedule"
  
  

kubectl create secret generic airflow-ssh-secret \
  --from-literal=gitSshKey='-----BEGIN OPENSSH PRIVATE KEY-----
b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW
QyNTUxOQAAACC46Ey1Qyjkl26NWAlG7v5VsayovUANpE6AyV88ookk8AAAAJhklC0MZJQt
DAAAAAtzc2gtZWQyNTUxOQAAACC46Ey1Qyjkl26NWAlG7v5VsayovUANpE6AyV88ookk8A
AAAEBjrAS5yAt4OmmiYE8sbg6V8FcIFnlWL6nEmir80YhcFLjoTLVDKOSXbo1YCUbu/lWx
rKi9QA2kToDJXzyiiSTwAAAAEGFpcmZsb3ctZ2l0LXN5bmMBAgMEBQ==
-----END OPENSSH PRIVATE KEY-----' \
  -n airflow
  
  
kubectl create secret generic airflow-ssh-knownhosts \
  --from-literal=knownHosts='github.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=' \
  -n airflow 
ChatGPT said:

√ìtimo pedido üöÄ

Vou converter toda a sequ√™ncia de comandos az cli + kubectl + helm em c√≥digo Terraform estruturado para rodar no HCP Terraform (antigo Terraform Cloud).

A solu√ß√£o ser√° dividida em m√≥dulos l√≥gicos:

    Resource Group + Network

    Postgres Flexible Server

    AKS Cluster + Node Pools

    DNS + Public IP

    Kubernetes Secrets

    Helm Release do Airflow

Aqui est√° a primeira vers√£o estruturada:
Terraform ‚Äì Infraestrutura Airflow no Azure

terraform {
  required_version = ">= 1.6.0"

  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 4.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.30"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.12"
    }
  }
}

provider "azurerm" {
  features {}
}

# ------------------------------
# 1. Resource Group + Network
# ------------------------------
resource "azurerm_resource_group" "airflow" {
  name     = "rg-airflow-prod"
  location = "Central US"
}

resource "azurerm_virtual_network" "airflow" {
  name                = "vnet-airflow"
  address_space       = ["10.10.0.0/16"]
  location            = azurerm_resource_group.airflow.location
  resource_group_name = azurerm_resource_group.airflow.name
}

resource "azurerm_subnet" "aks" {
  name                 = "subnet-aks"
  resource_group_name  = azurerm_resource_group.airflow.name
  virtual_network_name = azurerm_virtual_network.airflow.name
  address_prefixes     = ["10.10.0.0/22"]
}

resource "azurerm_subnet" "db" {
  name                 = "subnet-db-private"
  resource_group_name  = azurerm_resource_group.airflow.name
  virtual_network_name = azurerm_virtual_network.airflow.name
  address_prefixes     = ["10.10.4.0/24"]
}

resource "azurerm_private_dns_zone" "postgres" {
  name                = "privatelink.postgres.database.azure.com"
  resource_group_name = azurerm_resource_group.airflow.name
}

# ------------------------------
# 2. PostgreSQL Flexible Server
# ------------------------------
resource "azurerm_postgresql_flexible_server" "airflow" {
  name                   = "airflow-metadb-postgresql"
  resource_group_name    = azurerm_resource_group.airflow.name
  location               = azurerm_resource_group.airflow.location
  administrator_login    = "airflow_user"
  administrator_password = "airflow_pass"
  version                = "16"

  sku_name   = "Standard_D2ds_v5"
  storage_mb = 32768
  zone       = "1"

  delegated_subnet_id = azurerm_subnet.db.id
  private_dns_zone_id = azurerm_private_dns_zone.postgres.id

  high_availability {
    mode = "Disabled"
  }

  authentication {
    password_auth_enabled = true
  }

  depends_on = [azurerm_subnet.db]
}

resource "azurerm_postgresql_flexible_server_database" "airflow" {
  name      = "airflow"
  server_id = azurerm_postgresql_flexible_server.airflow.id
  charset   = "UTF8"
  collation = "en_US.utf8"
}

# ------------------------------
# 3. AKS Cluster + Node Pools
# ------------------------------
resource "azurerm_kubernetes_cluster" "airflow" {
  name                = "aks-airflow-prod"
  location            = azurerm_resource_group.airflow.location
  resource_group_name = azurerm_resource_group.airflow.name
  dns_prefix          = "airflow"

  default_node_pool {
    name                = "systempool"
    node_count          = 1
    vm_size             = "Standard_B2s"
    vnet_subnet_id      = azurerm_subnet.aks.id
    enable_auto_scaling = true
    min_count           = 1
    max_count           = 1
  }

  identity {
    type = "SystemAssigned"
  }

  kubernetes_version = "1.30.3"

  addon_profile {
    oms_agent {
      enabled = true
    }
  }
}

resource "azurerm_kubernetes_cluster_node_pool" "workerpool" {
  name                  = "workerpool"
  kubernetes_cluster_id = azurerm_kubernetes_cluster.airflow.id
  vm_size               = "Standard_D4s_v3"
  node_count            = 0
  enable_auto_scaling   = true
  min_count             = 0
  max_count             = 5
  mode                  = "User"

  node_taints = ["workload=worker:NoSchedule"]
}

# ------------------------------
# 4. Public IP para Airflow Web
# ------------------------------
resource "azurerm_public_ip" "airflow_web" {
  name                = "pip-airflow-web"
  location            = azurerm_resource_group.airflow.location
  resource_group_name = azurerm_kubernetes_cluster.airflow.node_resource_group
  allocation_method   = "Static"
  sku                 = "Standard"
}

# ------------------------------
# 5. Providers Kubernetes/Helm
# ------------------------------
provider "kubernetes" {
  host                   = azurerm_kubernetes_cluster.airflow.kube_config[0].host
  client_certificate     = base64decode(azurerm_kubernetes_cluster.airflow.kube_config[0].client_certificate)
  client_key             = base64decode(azurerm_kubernetes_cluster.airflow.kube_config[0].client_key)
  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.airflow.kube_config[0].cluster_ca_certificate)
}

provider "helm" {
  kubernetes {
    host                   = azurerm_kubernetes_cluster.airflow.kube_config[0].host
    client_certificate     = base64decode(azurerm_kubernetes_cluster.airflow.kube_config[0].client_certificate)
    client_key             = base64decode(azurerm_kubernetes_cluster.airflow.kube_config[0].client_key)
    cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.airflow.kube_config[0].cluster_ca_certificate)
  }
}

# ------------------------------
# 6. Secrets no Kubernetes
# ------------------------------
resource "kubernetes_namespace" "airflow" {
  metadata {
    name = "airflow"
  }
}

resource "kubernetes_secret" "db" {
  metadata {
    name      = "airflow-db-secret"
    namespace = kubernetes_namespace.airflow.metadata[0].name
  }

  data = {
    connection = base64encode("postgresql://airflow_user:airflow_pass@${azurerm_postgresql_flexible_server.airflow.fqdn}:5432/airflow")
  }
}

resource "kubernetes_secret" "ssh_key" {
  metadata {
    name      = "airflow-ssh-secret"
    namespace = kubernetes_namespace.airflow.metadata[0].name
  }

  data = {
    gitSshKey = filebase64("${path.module}/id_rsa") # chave privada local
  }
}

resource "kubernetes_secret" "ssh_knownhosts" {
  metadata {
    name      = "airflow-ssh-knownhosts"
    namespace = kubernetes_namespace.airflow.metadata[0].name
  }

  data = {
    knownHosts = base64encode("github.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=")
  }
}

# ------------------------------
# 7. Helm Chart Airflow
# ------------------------------
resource "helm_release" "airflow" {
  name       = "airflow"
  repository = "https://airflow.apache.org"
  chart      = "airflow"
  namespace  = kubernetes_namespace.airflow.metadata[0].name
  version    = "1.16.0"

  set {
    name  = "executor"
    value = "KubernetesExecutor"
  }

  set {
    name  = "postgresql.enabled"
    value = "false"
  }

  set {
    name  = "redis.enabled"
    value = "false"
  }

  set {
    name  = "data.metadataSecretName"
    value = kubernetes_secret.db.metadata[0].name
  }

  set {
    name  = "airflow.persistence.enabled"
    value = "true"
  }

  set {
    name  = "airflow.persistence.size"
    value = "20Gi"
  }

  set {
    name  = "images.airflow.repository"
    value = "apache/airflow"
  }

  set {
    name  = "images.airflow.tag"
    value = "2.10.5"
  }

  set {
    name  = "airflow.airflowVersion"
    value = "2.10.5"
  }

  set {
    name  = "webserver.service.type"
    value = "LoadBalancer"
  }

  set {
    name  = "webserver.service.loadBalancerIP"
    value = azurerm_public_ip.airflow_web.ip_address
  }

  set {
    name  = "dags.gitSync.enabled"
    value = "true"
  }

  set {
    name  = "dags.gitSync.repo"
    value = "git@github.com:danielkakazu/dags.git"
  }

  set {
    name  = "dags.gitSync.branch"
    value = "main"
  }

  set {
    name  = "dags.gitSync.subPath"
    value = "dags"
  }

  set {
    name  = "dags.gitSync.sshKeySecret"
    value = kubernetes_secret.ssh_key.metadata[0].name
  }

  set {
    name  = "dags.gitSync.knownHosts"
    value = "github.com"
  }

  set {
    name  = "executorConfig.nodeSelector.agentpool"
    value = "workerpool"
  }
}

üëâ Observa√ß√µes importantes:

    A chave SSH usada no secret airflow-ssh-secret deve estar salva em um arquivo (id_rsa) no diret√≥rio do m√≥dulo Terraform.

    O azurerm_kubernetes_cluster.airflow.node_resource_group √© um RG gerado automaticamente pelo AKS (necess√°rio para o Public IP).

    Os comandos kubectl patch foram substitu√≠dos por configura√ß√£o direta no helm_release.

    Os secrets de banco e SSH est√£o sendo injetados pelo Terraform.

Voc√™ quer que eu tamb√©m monte o pipeline HCP Terraform (workspace + vari√°veis sens√≠veis como senha do banco/SSH key) para voc√™ rodar isso direto no HCP?
You said:
sim por favor, e fa√ßa tudo isso gerando uma estrutura com arquivos zipados.
ChatGPT said:

Pronto ‚Äî gerei a estrutura completa de arquivos Terraform + workflow e empacotei tudo num ZIP para voc√™.

üì¶ Baixe o projeto (ZIP)

O ZIP cont√©m:

    providers.tf, main.tf, variables.tf, outputs.tf

    terraform.tfvars.example (exemplo de vari√°veis)

    Kubernetes secrets criados via provider Kubernetes (injetados do var.ssh_private_key e var.db_admin_password)

    scripts/deploy_local.sh (helper para execu√ß√£o local)

    .github/workflows/ci.yml (GitHub Actions que usa Terraform Cloud / HCP)

    README.md e README-HCP-VARS.md com instru√ß√µes para configurar o workspace no HCP e vari√°veis sens√≠veis

‚ö†Ô∏è Observa√ß√µes importantes (leia antes de aplicar):

    Substitua organization = "your-org" e workspaces.name em providers.tf pelo seu org/workspace no HCP.

    N√£o coloque segredos em reposit√≥rios p√∫blicos. Configure db_admin_password e ssh_private_key como sensitive variables no workspace do Terraform Cloud/HCP.

    O provider Kubernetes e Helm s√£o configurados via kube_config do recurso azurerm_kubernetes_cluster. Se o fluxo for executar no HCP, a run do workspace precisa ter permiss√£o/credenciais Azure (env vars AZURE_CLIENT_ID etc.) configuradas no workspace.

    Teste em um ambiente de dev antes de aplicar em produ√ß√£o (endere√ßos IP, tamanhos de VM, tags e quotas podem precisar de ajuste).

Se quiser, eu:

    atualizo os arquivos para incluir m√≥dulos separados (network / db / aks) e outputs mais detalhados;

    adiciono um exemplo de .tfvars populado (com placeholders) ou importo a chave SSH que voc√™ usou anteriormente para dentro do projeto (colocando como arquivo);

    ou j√° crio o workspace no HCP pra voc√™ (preciso apenas do nome da organiza√ß√£o e token TF API ‚Äî n√£o envie aqui se for sens√≠vel; em vez disso, posso fornecer o curl/gh/az cli exato para voc√™ executar).

O que prefere que eu fa√ßa agora?
You said:
Initializing HCP Terraform...
‚ï∑
‚îÇ Error: Terraform encountered problems during initialisation, including problems
‚îÇ with the configuration, described below.
‚îÇ 
‚îÇ The Terraform configuration must be valid before initialization so that
‚îÇ Terraform can determine which modules and providers need to be installed.
‚îÇ 
‚îÇ 
‚ïµ
‚ï∑
‚îÇ Error: Invalid expression
‚îÇ 
‚îÇ   on main.tf line 264, in resource "helm_release" "airflow":
‚îÇ  264:       set[?name=="webserver.service.loadBalancerIP"]
‚îÇ 
‚îÇ Expected the start of an expression, but found an invalid expression token.
‚ïµ
ChatGPT said:

Boa captura üëå

Esse erro vem da parte de lifecycle.ignore_changes no helm_release.
O que eu escrevi (set[?name=="webserver.service.loadBalancerIP"]) √© sintaxe de filtro HCL2 inv√°lida ‚Äî Terraform n√£o entende isso.

A forma correta de ignorar uma set { ... } do helm_release √© ignorar pelo campo todo, n√£o por sub-atributo.
Corre√ß√£o:

Troque este trecho no main.tf:

lifecycle {
  ignore_changes = [
    # prevent helm from forcing changes on dynamic fields like loadBalancer IP while AKS allocates it
    set[?name=="webserver.service.loadBalancerIP"]
  ]
}

por:

lifecycle {
  ignore_changes = [
    set
  ]
}

üëâ Isso faz com que o Terraform n√£o tente reconciliar qualquer mudan√ßa de set no chart (ou seja, se o Helm mudar algo em runtime, n√£o d√° conflito).
Se quiser ser mais granular (s√≥ para loadBalancerIP), a alternativa √© n√£o usar ignore_changes e deixar que o IP seja gerenciado pela Public IP do Azure, como j√° est√° configurado em azurerm_public_ip.airflow_web.

‚ö° Resumindo:

    Abra main.tf.

    Encontre o recurso helm_release "airflow".

    Substitua o bloco lifecycle pelo corrigido acima.

Depois rode novamente:

terraform init -upgrade
terraform plan

Quer que eu j√° te gere uma vers√£o corrigida do ZIP com esse ajuste pronto, para voc√™ baixar e testar direto?
You said:
Error: No value for required variable
on variables.tf line 21:
variable "db_admin_password" {
The root module input variable "db_admin_password" is not set, and has no default value. Use a -var or -var-file command line argument to provide a value for this variable.
Error: No value for required variable
on variables.tf line 27:
variable "ssh_private_key" {
The root module input variable "ssh_private_key" is not set, and has no default value. Use a -var or -var-file command line argument to provide a value for this variable.
ChatGPT said:

Perfeito üöÄ isso significa que o Terraform est√° reclamando de duas vari√°veis obrigat√≥rias que voc√™ mesmo precisa fornecer:

    db_admin_password ‚Üí senha do banco PostgreSQL (n√£o tem default, precisa ser definida).

    ssh_private_key ‚Üí chave privada usada para o GitSync do Airflow.

3 maneiras de resolver
1. Usando terraform.tfvars

Crie um arquivo terraform.tfvars na raiz do projeto com:

db_admin_password = "SenhaForteAqui123!"
ssh_private_key   = <<EOF
-----BEGIN OPENSSH PRIVATE KEY-----
cole sua chave privada aqui
-----END OPENSSH PRIVATE KEY-----
EOF

E depois rode normalmente:

terraform plan
terraform apply

2. Passando com -var

terraform apply \
  -var="db_admin_password=SenhaForteAqui123!" \
  -var="ssh_private_key=$(cat ~/.ssh/id_rsa)"

3. No HCP Terraform (Terraform Cloud)

No workspace que voc√™ criou no HCP:

    V√° em Variables ‚Üí Terraform Variables

        Adicione db_admin_password (Sensitive = ‚úÖ)

        Adicione ssh_private_key (Sensitive = ‚úÖ)

‚ö†Ô∏è Nunca coloque esses valores direto nos arquivos versionados (commit) ‚Äî s√≥ local ou no HCP com vari√°veis sens√≠veis.

üëâ Depois que voc√™ adicionar essas vari√°veis, o erro desaparece e o Terraform conseguir√° criar o PostgreSQL e os Secrets no Kubernetes.

Quer que eu prepare para voc√™ um terraform.tfvars j√° pronto (com placeholders) e gere um novo ZIP corrigido incluindo isso?
You said:
Error: "sku_name" is not a valid sku name, got Standard_D2ds_v5
with azurerm_postgresql_flexible_server.airflow_db
on main.tf line 43, in resource "azurerm_postgresql_flexible_server" "airflow_db":
  sku_name   = "Standard_D2ds_v5"
Error: expected high_availability.0.mode to be one of ["ZoneRedundant" "SameZone"], got Disabled
with azurerm_postgresql_flexible_server.airflow_db
on main.tf line 50, in resource "azurerm_postgresql_flexible_server" "airflow_db":
    mode = "Disabled"
Error: Unsupported block type
on main.tf line 90, in resource "azurerm_kubernetes_cluster" "aks":
  addon_profile {
Blocks of type "addon_profile" are not expected here.
Error: Unsupported argument
on main.tf line 79, in resource "azurerm_kubernetes_cluster" "aks":
    enable_auto_scaling = true
An argument named "enable_auto_scaling" is not expected here.
ChatGPT said:

Boa üëÄ ‚Äî voc√™ esbarrou em diferen√ßas entre o CLI (az) e o provider Terraform azurerm.
Os argumentos que o CLI aceita n√£o s√£o exatamente os mesmos que o Terraform entende. Vou corrigir ponto por ponto:
1. PostgreSQL Flexible Server

No Terraform, o campo sku_name n√£o usa a nomenclatura Standard_D2ds_v5.
Ele √© composto assim:

{tier}_{family}_{cores}

Exemplo v√°lido (equivalente a um D2ds v5 do CLI seria 2 vCores, General Purpose):

sku_name = "GP_Standard_D2ds_v5_2"

Mas a forma mais est√°vel √© usar um SKU suportado oficialmente no provider, tipo:

sku_name = "GP_Standard_D2s_v3_2"

‚ö†Ô∏è Sugiro voc√™ ajustar para algo mais seguro/testado, como GP_Standard_D2s_v3_2.
2. High Availability

No Terraform, n√£o existe Disabled. Se n√£o quiser HA, simplesmente n√£o declara o bloco high_availability.

Ou seja, remova:

high_availability {
  mode = "Disabled"
}

3. AKS Addons

No azurerm_kubernetes_cluster, n√£o existe mais addon_profile (foi deprectado).
O equivalente para habilitar Monitoramento √© via oms_agent dentro de monitor_metrics ou habilitando oms_agent direto.

Exemplo atualizado:

oms_agent {
  log_analytics_workspace_id = azurerm_log_analytics_workspace.law.id
}

Se voc√™ n√£o configurou Log Analytics, pode deixar esse bloco fora at√© criar o recurso.
4. Cluster Autoscaler

No Terraform, o autoscaler √© configurado dentro do default_node_pool, n√£o no cluster root.

Errado (o que voc√™ tinha):

resource "azurerm_kubernetes_cluster" "aks" {
  ...
  enable_auto_scaling = true
}

Certo:

default_node_pool {
  name                = "systempool"
  node_count          = 1
  vm_size             = "Standard_B2s"
  enable_auto_scaling = true
  min_count           = 1
  max_count           = 1
}

‚úÖ Resumindo as corre√ß√µes que voc√™ precisa aplicar:

    Trocar sku_name = "Standard_D2ds_v5" ‚Üí sku_name = "GP_Standard_D2s_v3_2".

    Remover o bloco high_availability se quiser sem HA.

    Apagar addon_profile { ... }.

    Mover enable_auto_scaling, min_count, max_count para dentro de default_node_pool.

Quer que eu j√° te entregue um novo ZIP corrigido com essas mudan√ßas aplicadas no main.tf?
You said:
este ultimo erro do auto_scaling esta no default_node_pool, 
# Resource Group and Network
resource "azurerm_resource_group" "airflow" {
  name     = var.resource_group_name
  location = var.location
}

resource "azurerm_virtual_network" "airflow" {
  name                = var.vnet_name
  address_space       = ["10.10.0.0/16"]
  location            = azurerm_resource_group.airflow.location
  resource_group_name = azurerm_resource_group.airflow.name
}

resource "azurerm_subnet" "aks" {
  name                 = "subnet-aks"
  resource_group_name  = azurerm_resource_group.airflow.name
  virtual_network_name = azurerm_virtual_network.airflow.name
  address_prefixes     = ["10.10.0.0/22"]
}

resource "azurerm_subnet" "db" {
  name                 = "subnet-db-private"
  resource_group_name  = azurerm_resource_group.airflow.name
  virtual_network_name = azurerm_virtual_network.airflow.name
  address_prefixes     = ["10.10.4.0/24"]
}

resource "azurerm_private_dns_zone" "postgres" {
  name                = "privatelink.postgres.database.azure.com"
  resource_group_name = azurerm_resource_group.airflow.name
}

# PostgreSQL Flexible Server (private)
resource "azurerm_postgresql_flexible_server" "airflow_db" {
  name                = "airflow-metadb-postgresql"
  resource_group_name = azurerm_resource_group.airflow.name
  location            = azurerm_resource_group.airflow.location

  administrator_login    = var.db_admin_user
  administrator_password = var.db_admin_password
  version                = "16"

  sku_name   = "GP_Standard_D2ds_v5"
  storage_mb = 32768

  delegated_subnet_id = azurerm_subnet.db.id
  private_dns_zone_id = azurerm_private_dns_zone.postgres.id

  authentication {
    password_auth_enabled = true
  }

  depends_on = [azurerm_subnet.db]
}

resource "azurerm_postgresql_flexible_server_database" "airflow_db_database" {
  name      = "airflow"
  server_id = azurerm_postgresql_flexible_server.airflow_db.id
  charset   = "UTF8"
  collation = "en_US.utf8"
}

# AKS cluster
resource "azurerm_kubernetes_cluster" "aks" {
  name                = "aks-airflow-prod"
  location            = azurerm_resource_group.airflow.location
  resource_group_name = azurerm_resource_group.airflow.name
  dns_prefix          = "airflow"

  default_node_pool {
    name                = "systempool"
    node_count          = 1
    vm_size             = "Standard_B2s"
    vnet_subnet_id      = azurerm_subnet.aks.id
    enable_auto_scaling = true
    min_count           = 1
    max_count           = 1
  }

  identity {
    type = "SystemAssigned"
  }

  kubernetes_version = "1.30.3"
}

resource "azurerm_kubernetes_cluster_node_pool" "workerpool" {
  name                  = "workerpool"
  kubernetes_cluster_id = azurerm_kubernetes_cluster.aks.id
  vm_size               = "Standard_D4s_v3"
  node_count            = 0
  enable_auto_scaling   = true
  min_count             = 0
  max_count             = 5
  mode                  = "User"

  node_taints = ["workload=worker:NoSchedule"]
}

# Public IP (in AKS node resource group)
resource "azurerm_public_ip" "airflow_web" {
  name                = "pip-airflow-web"
  location            = azurerm_resource_group.airflow.location
  resource_group_name = azurerm_kubernetes_cluster.aks.node_resource_group
  allocation_method   = "Static"
  sku                 = "Standard"
}

# Kubernetes namespace and secrets (using kubernetes provider configured from AKS kubeconfig)
resource "kubernetes_namespace" "airflow_ns" {
  metadata {
    name = "airflow"
  }

  depends_on = [azurerm_kubernetes_cluster.aks]
}

resource "kubernetes_secret" "airflow_db_secret" {
  metadata {
    name      = "airflow-db-secret"
    namespace = kubernetes_namespace.airflow_ns.metadata[0].name
  }

  data = {
    connection = base64encode("postgresql://${var.db_admin_user}:${var.db_admin_password}@${azurerm_postgresql_flexible_server.airflow_db.fqdn}:5432/airflow")
  }

  depends_on = [kubernetes_namespace.airflow_ns]
}

resource "kubernetes_secret" "airflow_ssh_secret" {
  metadata {
    name      = "airflow-ssh-secret"
    namespace = kubernetes_namespace.airflow_ns.metadata[0].name
  }

  data = {
    gitSshKey = base64encode(var.ssh_private_key)
  }

  depends_on = [kubernetes_namespace.airflow_ns]
}

resource "kubernetes_secret" "airflow_ssh_knownhosts" {
  metadata {
    name      = "airflow-ssh-knownhosts"
    namespace = kubernetes_namespace.airflow_ns.metadata[0].name
  }

  data = {
    knownHosts = base64encode("github.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCj7ndNxQowgcQnjshcLrqPEiiphnt+VTTvDP6mHBL9j1aNUkY4Ue1gvwnGLVlOhGeYrnZaMgRK6+PKCUXaDbC7qtbW8gIkhL7aGCsOr/C56SJMy/BCZfxd1nWzAOxSDPgVsmerOBYfNqltV9/hWCqBywINIR+5dIg6JTJ72pcEpEjcYgXkE2YEFXV1JHnsKgbLWNlhScqb2UmyRkQyytRLtL+38TGxkxCflmO+5Z8CSSNY7GidjMIZ7Q4zMjA2n1nGrlTDkzwDCsw+wqFPGQA179cnfGWOWRVruj16z6XyvxvjJwbz0wQZ75XK5tKSb7FNyeIEs4TT4jk+S4dhPeAUC5y+bDYirYgM4GC7uEnztnZyaVWQ7B381AK4Qdrwt51ZqExKbQpTUNn+EjqoTwvqNj4kqx5QUCI0ThS/YkOxJCXmPUWZbhjpCg56i+2aB6CmK2JGhn57K5mj0MNdBXA4/WnwH6XoPWJzK5Nyu2zB3nAZp+S5hpQs+p1vN1/wsjk=")
  }

  depends_on = [kubernetes_namespace.airflow_ns]
}

# Helm release for Airflow
resource "helm_release" "airflow" {
  name       = "airflow"
  repository = "https://airflow.apache.org"
  chart      = "airflow"
  namespace  = kubernetes_namespace.airflow_ns.metadata[0].name
  version    = "1.16.0"

  set {
    name  = "executor"
    value = "KubernetesExecutor"
  }

  set {
    name  = "postgresql.enabled"
    value = "false"
  }

  set {
    name  = "redis.enabled"
    value = "false"
  }

  set {
    name  = "data.metadataSecretName"
    value = kubernetes_secret.airflow_db_secret.metadata[0].name
  }

  set {
    name  = "airflow.persistence.enabled"
    value = "true"
  }

  set {
    name  = "airflow.persistence.size"
    value = "20Gi"
  }

  set {
    name  = "images.airflow.repository"
    value = "apache/airflow"
  }

  set {
    name  = "images.airflow.tag"
    value = var.airflow_image_tag
  }

  set {
    name  = "airflow.airflowVersion"
    value = var.airflow_image_tag
  }

  set {
    name  = "webserver.service.type"
    value = "LoadBalancer"
  }

  set {
    name  = "webserver.service.loadBalancerIP"
    value = azurerm_public_ip.airflow_web.ip_address
  }

  set {
    name  = "dags.gitSync.enabled"
    value = "true"
  }

  set {
    name  = "dags.gitSync.repo"
    value = var.dags_git_repo
  }

  set {
    name  = "dags.gitSync.branch"
    value = var.dags_git_branch
  }

  set {
    name  = "dags.gitSync.subPath"
    value = "dags"
  }

  set {
    name  = "dags.gitSync.sshKeySecret"
    value = kubernetes_secret.airflow_ssh_secret.metadata[0].name
  }

  # node selector and tolerations should match the workerpool
  set {
    name  = "executorConfig.nodeSelector.agentpool"
    value = "workerpool"
  }

  lifecycle {
    ignore_changes = [
      set
    ]
  }

  depends_on = [
    kubernetes_secret.airflow_db_secret,
    kubernetes_secret.airflow_ssh_secret,
    azurerm_public_ip.airflow_web
  ]
}
ChatGPT said:

Show, agora consigo ver tudo que voc√™ tem no main.tf üëå.
O problema que voc√™ encontrou com enable_auto_scaling n√£o √© que est√° ‚Äúfora do lugar‚Äù, mas sim que a forma de declarar mudou entre vers√µes do provider azurerm.
üìå Problema

    No recurso azurerm_kubernetes_cluster.default_node_pool voc√™ escreveu:

default_node_pool {
  name                = "systempool"
  node_count          = 1
  vm_size             = "Standard_B2s"
  vnet_subnet_id      = azurerm_subnet.aks.id
  enable_auto_scaling = true
  min_count           = 1
  max_count           = 1
}

O erro do Terraform (enable_auto_scaling n√£o esperado) indica que a vers√£o do provider azurerm que voc√™ est√° usando n√£o aceita mais esse campo dentro do default_node_pool.
‚úÖ Corre√ß√£o (2 op√ß√µes)
Op√ß√£o 1 ‚Äì Fixar vers√£o do provider

Se voc√™ quer manter enable_auto_scaling, precisa usar azurerm >= 3.38.
No seu providers.tf:

terraform {
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = ">=3.38.0"
    }
  }
}

provider "azurerm" {
  features {}
}

Com essa vers√£o, o bloco acima funciona normalmente.
Op√ß√£o 2 ‚Äì Usar azurerm_kubernetes_cluster_node_pool para autoscaling

Se voc√™ n√£o quiser atualizar, deixe o default_node_pool fixo e configure o autoscaling apenas no node pool adicional:

default_node_pool {
  name           = "systempool"
  node_count     = 1
  vm_size        = "Standard_B2s"
  vnet_subnet_id = azurerm_subnet.aks.id
}

resource "azurerm_kubernetes_cluster_node_pool" "system_autoscaler" {
  name                  = "systemautoscaler"
  kubernetes_cluster_id = azurerm_kubernetes_cluster.aks.id
  vm_size               = "Standard_B2s"
  enable_auto_scaling   = true
  min_count             = 1
  max_count             = 3
  mode                  = "System"
}

üîß Al√©m disso:

    Troque sku_name = "GP_Standard_D2ds_v5" para algo v√°lido, ex:

    sku_name = "GP_Standard_D2s_v3_2"

    Remova o bloco high_availability do PostgreSQL, j√° que "Disabled" n√£o existe em Terraform.

    O addon_profile precisa ser removido mesmo, como j√° comentamos.

üëâ Pergunto: voc√™ prefere que eu atualize seu c√≥digo para usar o provider azurerm >= 3.38 (mais simples, manter enable_auto_scaling onde est√°),
ou prefere que eu reorganize os nodepools (sem atualizar provider, mas mais verboso)?
You said:
Error: Unsupported block type
on main.tf line 162, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 167, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 172, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 177, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 182, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 187, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 192, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 197, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 202, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 207, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 212, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 217, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 222, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 227, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 232, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 237, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
Error: Unsupported block type
on main.tf line 243, in resource "helm_release" "airflow":
  set {
Blocks of type "set" are not expected here. Did you mean to define argument "set"? If so, use the equals sign to assign it a value.
ChatGPT said:

Ah, agora o erro √© totalmente compreens√≠vel: o provider helm que voc√™ est√° usando mudou a forma de passar valores set.

Na vers√£o >= 2.x do provider helm, o bloco set { ... } n√£o √© mais suportado como bloco, voc√™ precisa usar argumento set com lista de mapas.
Corre√ß√£o do helm_release

Troque isso (sintaxe antiga):

resource "helm_release" "airflow" {
  name       = "airflow"
  repository = "https://airflow.apache.org"
  chart      = "airflow"
  namespace  = kubernetes_namespace.airflow_ns.metadata[0].name
  version    = "1.16.0"

  set {
    name  = "executor"
    value = "KubernetesExecutor"
  }

  set {
    name  = "postgresql.enabled"
    value = "false"
  }

  ...
}

Por isso (sintaxe nova compat√≠vel com helm >= 2):

resource "helm_release" "airflow" {
  name       = "airflow"
  repository = "https://airflow.apache.org"
  chart      = "airflow"
  namespace  = kubernetes_namespace.airflow_ns.metadata[0].name
  version    = "1.16.0"

  set = [
    {
      name  = "executor"
      value = "KubernetesExecutor"
    },
    {
      name  = "postgresql.enabled"
      value = "false"
    },
    {
      name  = "redis.enabled"
      value = "false"
    },
    {
      name  = "data.metadataSecretName"
      value = kubernetes_secret.airflow_db_secret.metadata[0].name
    },
    {
      name  = "airflow.persistence.enabled"
      value = "true"
    },
    {
      name  = "airflow.persistence.size"
      value = "20Gi"
    },
    {
      name  = "images.airflow.repository"
      value = "apache/airflow"
    },
    {
      name  = "images.airflow.tag"
      value = var.airflow_image_tag
    },
    {
      name  = "airflow.airflowVersion"
      value = var.airflow_image_tag
    },
    {
      name  = "webserver.service.type"
      value = "LoadBalancer"
    },
    {
      name  = "webserver.service.loadBalancerIP"
      value = azurerm_public_ip.airflow_web.ip_address
    },
    {
      name  = "dags.gitSync.enabled"
      value = "true"
    },
    {
      name  = "dags.gitSync.repo"
      value = var.dags_git_repo
    },
    {
      name  = "dags.gitSync.branch"
      value = var.dags_git_branch
    },
    {
      name  = "dags.gitSync.subPath"
      value = "dags"
    },
    {
      name  = "dags.gitSync.sshKeySecret"
      value = kubernetes_secret.airflow_ssh_secret.metadata[0].name
    },
    {
      name  = "executorConfig.nodeSelector.agentpool"
      value = "workerpool"
    }
  ]

  lifecycle {
    ignore_changes = [
      set
    ]
  }

  depends_on = [
    kubernetes_secret.airflow_db_secret,
    kubernetes_secret.airflow_ssh_secret,
    azurerm_public_ip.airflow_web
  ]
}

‚úÖ Explica√ß√£o:

    set agora √© um argumento √∫nico, recebe uma lista de mapas com name e value.

    Blocos individuais set { ... } n√£o existem mais no Terraform 1.6 + Helm provider 2.x.

    lifecycle.ignore_changes = [set] continua v√°lido para ignorar altera√ß√µes din√¢micas.

Se quiser, posso gerar uma vers√£o completa do main.tf corrigida pronta para rodar no HCP com:

    Helm provider 2.x

    PostgreSQL SKU e HA v√°lidos

    AKS com nodepool autoscaling correto

    Removido blocos/deprecated fields

Isso resolve todos os erros que voc√™ encontrou at√© agora.

Quer que eu fa√ßa isso?
You said:
Error: Post "http://localhost/api/v1/namespaces": dial tcp [::1]:80: connect: connection refused
with kubernetes_namespace.airflow_ns
on main.tf line 107, in resource "kubernetes_namespace" "airflow_ns":
resource "kubernetes_namespace" "airflow_ns" {
Error: creating Flexible Server (Subscription: "8cd5fdce-6161-4fb3-80f1-de5dd3736921" Resource Group Name: "rg-airflow-prod" Flexible Server Name: "airflow-metadb-postgresql"): performing Create: unexpected status 400 (400 Bad Request) with error: ConflictingPublicNetworkAccessAndVirtualNetworkConfiguration: Conflicting configuration is detected between Public Network Access and Virtual Network arguments. Public Network Access is not supported along with Virtual Network feature.
with azurerm_postgresql_flexible_server.airflow_db
on main.tf line 34, in resource "azurerm_postgresql_flexible_server" "airflow_db":
resource "azurerm_postgresql_flexible_server" "airflow_db" {
ChatGPT said:

üí° Excelente, agora voc√™ pegou dois pontos diferentes:
1. Erro do Kubernetes provider (dial tcp [::1]:80)

Isso acontece porque o provider kubernetes est√° tentando conectar no cluster AKS antes do cluster estar pronto.
Por padr√£o ele vai usar http://localhost:80 porque n√£o recebeu kubeconfig.

üëâ Solu√ß√µes poss√≠veis:

    Configure o provider kubernetes para usar as credenciais do AKS:

provider "kubernetes" {
  host                   = azurerm_kubernetes_cluster.aks.kube_config[0].host
  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_certificate)
  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_key)
  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].cluster_ca_certificate)
}

    E coloque depends_on = [azurerm_kubernetes_cluster.aks] nos recursos Kubernetes (voc√™ j√° fez em alguns).

Assim o Terraform s√≥ tenta aplicar os manifests depois que o cluster existe e o kubeconfig foi gerado.
2. Erro do PostgreSQL Flexible Server (ConflictingPublicNetworkAccessAndVirtualNetworkConfiguration)

Esse erro vem da API do Azure: voc√™ definiu delegated_subnet_id e private_dns_zone_id (modo Private Access), mas n√£o desativou o acesso p√∫blico.

üëâ Corre√ß√£o: adicione ao seu recurso:

resource "azurerm_postgresql_flexible_server" "airflow_db" {
  name                = "airflow-metadb-postgresql"
  resource_group_name = azurerm_resource_group.airflow.name
  location            = azurerm_resource_group.airflow.location

  administrator_login    = var.db_admin_user
  administrator_password = var.db_admin_password
  version                = "16"

  sku_name   = "GP_Standard_D2s_v3_2"
  storage_mb = 32768

  delegated_subnet_id    = azurerm_subnet.db.id
  private_dns_zone_id    = azurerm_private_dns_zone.postgres.id
  public_network_access_enabled = false   # üî• ESSENCIAL

  authentication {
    password_auth_enabled = true
  }

  depends_on = [azurerm_subnet.db]
}

Isso deixa o servidor 100% privado, acess√≠vel s√≥ pelo subnet e DNS privado.

‚úÖ Resumindo:

    Corrija o provider Kubernetes para usar o kube_config do AKS.

    Adicione public_network_access_enabled = false ao PostgreSQL Flexible Server.

Quer que eu j√° te gere um ZIP atualizado com:

    provider Kubernetes configurado,

    PostgreSQL privado corrigido,

    Helm syntax corrigida,

    autoscaling/sku v√°lidos?

You said:
do kube_config eu n√£o entendi, pode me dizer detalhadamente onde devo trocar e o que ?
ChatGPT said:

Boa! Esse ponto do kube_config realmente √© confuso no AKS quando usado com Terraform. Vou detalhar passo a passo, pra voc√™ n√£o se perder:
üìç Onde voc√™ est√° agora

    Voc√™ criou o recurso azurerm_kubernetes_cluster "aks".

    Em seguida est√° tentando criar kubernetes_namespace e kubernetes_secret usando o provider kubernetes.

    Mas o provider kubernetes n√£o sabe como se conectar ao cluster, ent√£o ele tentou http://localhost:80 ‚Üí erro connection refused.

üìç Como resolver

O provider kubernetes precisa receber as credenciais do AKS. Essas credenciais v√™m direto do recurso azurerm_kubernetes_cluster.
1. Edite o providers.tf

Adicione (ou substitua) o bloco do provider kubernetes:

provider "kubernetes" {
  host                   = azurerm_kubernetes_cluster.aks.kube_config[0].host
  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_certificate)
  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_key)
  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].cluster_ca_certificate)
}

‚ö†Ô∏è Importante:

    Esse kube_config √© gerado pelo Azure automaticamente quando voc√™ cria o cluster.

    O base64decode √© necess√°rio porque o Terraform armazena os certificados codificados em Base64.

2. Garanta a ordem de cria√ß√£o

O provider kubernetes s√≥ funciona depois que o cluster existe.
Ent√£o, nos recursos que usam Kubernetes (namespace, secrets, helm_release), coloque:

depends_on = [azurerm_kubernetes_cluster.aks]

Voc√™ j√° fez em alguns (namespace, secret), mas pode refor√ßar em todos que dependem do cluster.
3. Exemplo pr√°tico

Seu providers.tf ficaria assim:

provider "azurerm" {
  features {}
}

provider "helm" {
  kubernetes {
    host                   = azurerm_kubernetes_cluster.aks.kube_config[0].host
    client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_certificate)
    client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_key)
    cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].cluster_ca_certificate)
  }
}

provider "kubernetes" {
  host                   = azurerm_kubernetes_cluster.aks.kube_config[0].host
  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_certificate)
  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_key)
  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].cluster_ca_certificate)
}

Assim:

    azurerm ‚Üí cria os recursos no Azure.

    kubernetes ‚Üí usa o kubeconfig embutido no recurso do AKS.

    helm ‚Üí instala o chart no AKS usando o mesmo kubeconfig.

üëâ Pergunto:
Quer que eu j√° prepare pra voc√™ um providers.tf completo + ajustes nos depends_on e te mando tudo em um ZIP revisado (com PostgreSQL privado tamb√©m corrigido)?
You said:
mas o meu bloco do  provider esta assim
terraform {
  required_version = ">= 1.6.0"

  backend "remote" {
    host = "app.terraform.io" # HCP / Terraform Cloud host
    organization = "your-org" # <-- substitua pelo nome da sua organiza√ß√£o no HCP

    workspaces {
      name = "airflow-azure-workspace" # <-- nome do workspace
    }
  }

  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = ">= 3.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = ">= 2.0"
    }
    helm = {
      source  = "hashicorp/helm"
      version = ">= 2.0"
    }
  }
}

provider "azurerm" {
  features {}
}

# Kubernetes and Helm providers are configured later using the AKS kubeconfig outputs
ChatGPT said:

Perfeito üëå obrigado por compartilhar ‚Äî isso explica a confus√£o.

O bloco que voc√™ mostrou (terraform { ... } + provider "azurerm") est√° s√≥ definindo quais providers usar, mas o kubernetes e o helm ainda n√£o est√£o configurados.

O coment√°rio no final do arquivo j√° d√° a dica:

# Kubernetes and Helm providers are configured later using the AKS kubeconfig outputs

Ou seja: voc√™ precisa criar providers adicionais que apontem para as credenciais (kube_config) do seu azurerm_kubernetes_cluster.aks.
‚úÖ Como corrigir

Logo abaixo desse bloco que voc√™ j√° tem, adicione:

provider "kubernetes" {
  host                   = azurerm_kubernetes_cluster.aks.kube_config[0].host
  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_certificate)
  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_key)
  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].cluster_ca_certificate)
}

provider "helm" {
  kubernetes {
    host                   = azurerm_kubernetes_cluster.aks.kube_config[0].host
    client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_certificate)
    client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_key)
    cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].cluster_ca_certificate)
  }
}

‚ö†Ô∏è Ordem de cria√ß√£o (muito importante)

Esses providers usam o output de azurerm_kubernetes_cluster.aks.
Ent√£o, se o Terraform tentar criar kubernetes_namespace ou helm_release antes do cluster existir, vai falhar.

üëâ Por isso, voc√™ precisa garantir depends_on nos recursos Kubernetes/Helm, assim:

resource "kubernetes_namespace" "airflow_ns" {
  metadata {
    name = "airflow"
  }

  depends_on = [azurerm_kubernetes_cluster.aks]
}

resource "helm_release" "airflow" {
  name       = "airflow"
  repository = "https://airflow.apache.org"
  chart      = "airflow"
  namespace  = kubernetes_namespace.airflow_ns.metadata[0].name

  depends_on = [
    kubernetes_namespace.airflow_ns,
    azurerm_kubernetes_cluster.aks
  ]
}

üìå Resumindo:

    Seu terraform { ... } fica como est√° (com os required_providers).

    Logo abaixo, voc√™ adiciona os blocos provider "kubernetes" e provider "helm" configurados com o kube_config.

    E refor√ßa depends_on em tudo que depende do AKS.

Quer que eu j√° te monte um providers.tf pronto com esse ajuste, junto com as depends_on adicionadas nos pontos certos do seu main.tf?
You said:
Error: Unsupported block type
on providers.tf line 41, in provider "helm":
  kubernetes {
Blocks of type "kubernetes" are not expected here. Did you mean to define argument "kubernetes"? If so, use the equals sign to assign it a value.
ChatGPT said:

Boa, esse erro faz sentido üëå

No provider helm, voc√™ n√£o usa bloco kubernetes {}, mas sim argumento kubernetes {} (inline map).
A sintaxe √© ligeiramente diferente.
üöÄ Corre√ß√£o

Troque o que voc√™ tem hoje:

provider "helm" {
  kubernetes {
    host                   = azurerm_kubernetes_cluster.aks.kube_config[0].host
    client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_certificate)
    client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_key)
    cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].cluster_ca_certificate)
  }
}

Por esta vers√£o v√°lida:

provider "helm" {
  kubernetes {
    host                   = azurerm_kubernetes_cluster.aks.kube_config[0].host
    client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_certificate)
    client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_key)
    cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].cluster_ca_certificate)
  }
}

üëâ Mas o detalhe √©: dependendo da vers√£o do provider helm, o suporte ao bloco kubernetes {} pode n√£o estar dispon√≠vel.
Se voc√™ estiver usando hashicorp/helm >= 2.9.0, a forma recomendada √© passar como argumento kubernetes {} exatamente como acima.
Se for uma vers√£o mais antiga (< 2.9.0), o correto √© usar um provider kubernetes separado e o helm pega esse provider automaticamente.
‚úÖ Solu√ß√£o mais compat√≠vel (independente da vers√£o)

O jeito mais seguro √© deixar o helm sem configura√ß√£o pr√≥pria e ele vai usar o mesmo provider "kubernetes" que voc√™ j√° configurou:

provider "helm" {
  kubernetes {
    config_path = "~/.kube/config"
  }
}

üëâ S√≥ que como voc√™ est√° pegando os dados direto do azurerm_kubernetes_cluster, o ideal √©:

    manter o provider "kubernetes" configurado com o kube_config do AKS

    e usar o helm sem argumentos extras, que ele herda do Kubernetes provider.

üìå Pergunto: voc√™ prefere que eu simplifique e deixe s√≥ provider "kubernetes" com o kube_config do AKS, e o helm pegando automaticamente esse provider, ou voc√™ quer manter o helm com a configura√ß√£o expl√≠cita?
You said:
Error: Unsupported block type
on providers.tf line 41, in provider "helm":
  kubernetes {
Blocks of type "kubernetes" are not expected here. Did you mean to define argument "kubernetes"? If so, use the equals sign to assign it a value.
ChatGPT said:

Perfeito, agora ficou claro o que est√° acontecendo üö¶

O erro:

Error: Unsupported block type
on providers.tf line 41, in provider "helm":
  kubernetes {
Blocks of type "kubernetes" are not expected here.

significa que a vers√£o do provider helm que voc√™ est√° usando n√£o aceita o bloco kubernetes { ... } dentro dele.
Isso mudou entre vers√µes antigas e novas do provider.
‚úÖ Como resolver

Na sua configura√ß√£o, j√° existe um provider "kubernetes" configurado.
Ent√£o, a forma correta √© n√£o configurar nada dentro do provider "helm" ‚Üí ele herda o contexto do provider kubernetes.
providers.tf corrigido:

terraform {
  required_version = ">= 1.6.0"

  backend "remote" {
    host         = "app.terraform.io"
    organization = "your-org"

    workspaces {
      name = "airflow-azure-workspace"
    }
  }

  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = ">= 3.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = ">= 2.0"
    }
    helm = {
      source  = "hashicorp/helm"
      version = ">= 2.0"
    }
  }
}

provider "azurerm" {
  features {}
}

provider "kubernetes" {
  host                   = azurerm_kubernetes_cluster.aks.kube_config[0].host
  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_certificate)
  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_key)
  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].cluster_ca_certificate)
}

provider "helm" {}

üîë Resumo

    provider "kubernetes" ‚Üí conecta no AKS usando os outputs de azurerm_kubernetes_cluster.

    provider "helm" ‚Üí n√£o precisa de nada, ele automaticamente usa o kubernetes.

üëâ Quer que eu j√° ajuste o seu main.tf para garantir que todos os recursos Helm (helm_release) e Kubernetes (namespace, secrets) tenham depends_on no AKS, e te mande um pacote .zip pronto corrigido?
You said:
Error: Get "http://localhost/api/v1/namespaces/airflow": dial tcp [::1]:80: connect: connection refused
with kubernetes_namespace.airflow_ns
on main.tf line 108, in resource "kubernetes_namespace" "airflow_ns":
resource "kubernetes_namespace" "airflow_ns" {
ChatGPT said:

Boa, esse erro j√° n√£o √© de sintaxe, mas sim de conex√£o:

Error: Get "http://localhost/api/v1/namespaces/airflow": dial tcp [::1]:80: connect: connection refused

O Terraform est√° tentando falar com o Kubernetes, mas em vez de usar o endpoint do AKS, ele est√° caindo no http://localhost:80 (ou seja, n√£o conseguiu pegar o kubeconfig do cluster ainda).
üîç Causas prov√°veis

    O provider "kubernetes" est√° sendo inicializado antes do cluster AKS ser criado.

    O bloco provider "kubernetes" n√£o est√° usando os outputs corretos de azurerm_kubernetes_cluster.

    Faltou depends_on para for√ßar que o AKS exista antes de usar o provider.

‚úÖ Como corrigir
1. Ajuste no provider kubernetes

Confirme que ele est√° assim no providers.tf:

provider "kubernetes" {
  host                   = azurerm_kubernetes_cluster.aks.kube_config[0].host
  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_certificate)
  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_key)
  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].cluster_ca_certificate)

  # garante que s√≥ inicializa depois do AKS
  load_config_file       = false
}

üëâ sem o load_config_file = false, ele tenta buscar ~/.kube/config e cai no localhost.
2. Adicione depends_on

Nos recursos que usam o Kubernetes provider (namespace, secrets, helm release), force depend√™ncia expl√≠cita:

resource "kubernetes_namespace" "airflow_ns" {
  metadata {
    name = "airflow"
  }

  depends_on = [azurerm_kubernetes_cluster.aks]
}

E o mesmo para os kubernetes_secret e helm_release.
3. Ordem de aplica√ß√£o

No primeiro terraform apply, pode ser necess√°rio rodar em duas fases:

    Criar AKS e rede:

terraform apply -target=azurerm_kubernetes_cluster.aks

Depois aplicar o resto normalmente:

    terraform apply

‚ö° Isso resolve porque garante que o provider "kubernetes" tenha as credenciais do AKS j√° provisionado.

üëâ Quer que eu te monte um providers.tf + main.tf corrigidos (com depends_on em todos os recursos Kubernetes/Helm) e te entregue num .zip j√° ajustado?
You said:
Error: Get "http://localhost/api/v1/namespaces/airflow": dial tcp [::1]:80: connect: connection refused
with kubernetes_namespace.airflow_ns
on main.tf line 108, in resource "kubernetes_namespace" "airflow_ns":
resource "kubernetes_namespace" "airflow_ns" {
ChatGPT said:

Perfeito, esse erro est√° te perseguindo porque o Terraform ainda tenta usar o localhost como endpoint do Kubernetes, em vez do endpoint do AKS. Isso acontece quando o provider "kubernetes" n√£o est√° corretamente ligado ao output do azurerm_kubernetes_cluster.
‚úÖ Como corrigir de forma robusta
1. Configure o kubernetes provider assim:

provider "kubernetes" {
  host                   = azurerm_kubernetes_cluster.aks.kube_config[0].host
  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_certificate)
  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_key)
  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].cluster_ca_certificate)

  load_config_file       = false
}

üëâ Esse load_config_file = false √© obrigat√≥rio.
Sem ele, o Terraform tenta usar ~/.kube/config e acaba conectando em http://localhost:80.
2. For√ßar ordem com depends_on

Todos os recursos que usam Kubernetes precisam esperar o cluster AKS estar pronto. Exemplo:

resource "kubernetes_namespace" "airflow_ns" {
  metadata {
    name = "airflow"
  }

  depends_on = [azurerm_kubernetes_cluster.aks]
}

E fa√ßa o mesmo para:

    kubernetes_secret.airflow_db_secret

    kubernetes_secret.airflow_ssh_secret

    kubernetes_secret.airflow_ssh_knownhosts

    helm_release.airflow

3. Aplicar em duas fases (primeira vez)

Mesmo com o depends_on, √†s vezes o AKS demora para liberar o kubeconfig.
Ent√£o, no primeiro apply, pode ser necess√°rio rodar em duas etapas:

terraform apply -target=azurerm_kubernetes_cluster.aks

Quando o cluster j√° existir, rode o restante:

terraform apply

üîë Resumo:

    O provider kubernetes precisa ser configurado com os blocos host, client_certificate, client_key, cluster_ca_certificate.

    load_config_file = false evita cair no localhost.

    depends_on garante que o AKS j√° exista antes de criar namespace, secret e helm_release.

    No primeiro deploy, talvez precise rodar em duas fases.

üëâ Quer que eu te entregue um providers.tf + main.tf revisados j√° com load_config_file = false e todos os depends_on para voc√™ aplicar sem cair no localhost?
You said:
Error: Unsupported argument
on providers.tf line 39, in provider "kubernetes":
  load_config_file       = false
An argument named "load_config_file" is not expected here.
ChatGPT said:

Boa üö¶ ‚Äî isso confirma que a vers√£o do provider kubernetes que voc√™ est√° usando n√£o aceita o argumento load_config_file.

Esse campo existia em vers√µes mais antigas (1.x e 2.x iniciais), mas nas vers√µes recentes (>=2.20) a HashiCorp mudou a forma de configurar o provider.
‚úÖ Como corrigir

Em vez de load_config_file, voc√™ s√≥ precisa definir os campos diretamente.
Exemplo compat√≠vel com as vers√µes atuais:

provider "kubernetes" {
  host                   = azurerm_kubernetes_cluster.aks.kube_config[0].host
  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_certificate)
  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_key)
  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].cluster_ca_certificate)
}

üëâ Ou seja, remova completamente load_config_file.
‚ö° Dica extra

Se ainda assim cair no localhost, pode ser que o Terraform esteja tentando inicializar o provider "kubernetes" antes de o AKS existir.
Nesse caso:

    Rode em duas etapas:

terraform apply -target=azurerm_kubernetes_cluster.aks

(isso cria o AKS primeiro)

Depois rode:

    terraform apply

    Adicione depends_on nos recursos Kubernetes e Helm para garantir ordem, por exemplo:

resource "kubernetes_namespace" "airflow_ns" {
  metadata {
    name = "airflow"
  }

  depends_on = [azurerm_kubernetes_cluster.aks]
}

üîë Resumo:

    Remova load_config_file (n√£o existe nessa vers√£o).

    Continue usando os atributos host, client_certificate, client_key, cluster_ca_certificate.

    Garanta que o AKS esteja criado antes do kubernetes provider ser usado (com depends_on ou terraform apply -target).

üëâ Quer que eu monte para voc√™ um providers.tf atualizado, sem load_config_file, j√° pronto para HCP Terraform?
You said:
Error: Get "http://localhost/api/v1/namespaces/airflow": dial tcp [::1]:80: connect: connection refused
with kubernetes_namespace.airflow_ns
on main.tf line 108, in resource "kubernetes_namespace" "airflow_ns":
resource "kubernetes_namespace" "airflow_ns" {
ChatGPT said:

Entendi üîé ‚Äî esse erro continua porque o provider kubernetes ainda n√£o est√° pegando as credenciais do AKS no momento em que o Terraform tenta criar o namespace.

O sintoma √© claro:

Get "http://localhost/api/v1/namespaces/airflow": dial tcp [::1]:80: connect: connection refused

üëâ Isso significa que o Terraform est√° caindo no localhost (default) em vez de usar os dados de azurerm_kubernetes_cluster.aks.kube_config.
‚úÖ Como resolver de forma garantida
1. Configure os providers com alias e depends_on

üìå providers.tf atualizado:

provider "azurerm" {
  features {}
}

# Provider Kubernetes, usando kube_config do AKS
provider "kubernetes" {
  alias                  = "aks"
  host                   = azurerm_kubernetes_cluster.aks.kube_config[0].host
  client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_certificate)
  client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_key)
  cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].cluster_ca_certificate)
}

# Provider Helm, tamb√©m apontando para o cluster AKS
provider "helm" {
  alias = "aks"
  kubernetes {
    host                   = azurerm_kubernetes_cluster.aks.kube_config[0].host
    client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_certificate)
    client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].client_key)
    cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config[0].cluster_ca_certificate)
  }
}

2. Use os providers explicitamente nos recursos

Por exemplo, no main.tf:

resource "kubernetes_namespace" "airflow_ns" {
  provider = kubernetes.aks

  metadata {
    name = "airflow"
  }

  depends_on = [azurerm_kubernetes_cluster.aks]
}

resource "kubernetes_secret" "airflow_db_secret" {
  provider = kubernetes.aks

  metadata {
    name      = "airflow-db-secret"
    namespace = kubernetes_namespace.airflow_ns.metadata[0].name
  }

  data = {
    connection = base64encode("postgresql://${var.db_admin_user}:${var.db_admin_password}@${azurerm_postgresql_flexible_server.airflow_db.fqdn}:5432/airflow")
  }

  depends_on = [kubernetes_namespace.airflow_ns]
}

resource "helm_release" "airflow" {
  provider = helm.aks

  name       = "airflow"
  repository = "https://airflow.apache.org"
  chart      = "airflow"
  namespace  = kubernetes_namespace.airflow_ns.metadata[0].name
  version    = "1.16.0"

  set = [
  {
    name  = "executor"
    value = "KubernetesExecutor"
  },
  {
    name  = "postgresql.enabled"
    value = "false"
  },
  {
    name  = "redis.enabled"
    value = "false"
  },
  {
    name  = "data.metadataSecretName"
    value = kubernetes_secret.airflow_db_secret.metadata[0].name
  },
  {
    name  = "airflow.persistence.enabled"
    value = "true"
  },
  {
    name  = "airflow.persistence.size"
    value = "20Gi"
  },
  {
    name  = "images.airflow.repository"
    value = "apache/airflow"
  },
  {
    name  = "images.airflow.tag"
    value = var.airflow_image_tag
  },
  {
    name  = "airflow.airflowVersion"
    value = var.airflow_image_tag
  },
  {
    name  = "webserver.service.type"
    value = "LoadBalancer"
  },
  {
    name  = "webserver.service.loadBalancerIP"
    value = azurerm_public_ip.airflow_web.ip_address
  },
  {
    name  = "dags.gitSync.enabled"
    value = "true"
  },
  {
    name  = "dags.gitSync.repo"
    value = var.dags_git_repo
  },
  {
    name  = "dags.gitSync.branch"
    value = var.dags_git_branch
  },
  {
    name  = "dags.gitSync.subPath"
    value = "dags"
  },
  {
    name  = "dags.gitSync.sshKeySecret"
    value = kubernetes_secret.airflow_ssh_secret.metadata[0].name
  },
  # node selector and tolerations should match the workerpool
  {
    name  = "executorConfig.nodeSelector.agentpool"
    value = "workerpool"
  }
]

  lifecycle {
    ignore_changes = [
      set
    ]
  }

  depends_on = [
    kubernetes_namespace.airflow_ns,
    azurerm_kubernetes_cluster.aks,
    kubernetes_secret.airflow_db_secret,
    kubernetes_secret.airflow_ssh_secret,
    azurerm_public_ip.airflow_web
  ]
}
